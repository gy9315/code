# natural_learning_process

## D0407

### 학습내용
텍스트 전처리의 기초인 토큰화, 불용어 제거, 어간 추출, 표제어 처리 등의 개념과 Python 기반 실습을 병행하며 다양한 형태의 문자열 처리 기법을 익힘.

### 개인 복습/실습
- nltk.word_tokenize, nltk.Text 등을 이용해 문장을 단어 단위로 분리하고 빈도 분석 수행
- stopwords 리스트를 직접 구성하거나 불용어 사전을 활용해 의미 없는 단어 제거
- PorterStemmer, LancasterStemmer, WordNetLemmatizer를 비교해 어간 추출과 표제어 처리 방식의 차이를 학습함
- '어간 추출과 표제어 처리의 차이를 정리해줘'라는 질문을 통해 둘의 목적과 결과 차이를 확인함
- 불용어 제거 전/후 텍스트 의미 비교 실험을 통해 실전 텍스트 분석에 미치는 영향을 체감함

### 회고
단어 수준의 단순 분리뿐 아니라 문맥상 중요하지 않은 단어를 제거하고, 형태를 정규화하는 전처리의 힘이 얼마나 중요한지를 처음으로 실감했다.

## D0408

### 학습내용
한글 형태소 분석 도구들(Konlpy, Okt, Komoran, jamo 등)을 비교하며 형태소 단위 분리, 품사 태깅, 자모 단위 분리 등 한국어 자연어처리 기초 기능을 실습함.

### 개인 복습/실습
- konlpy의 Okt, Komoran, Hannanum 모듈로 문장을 형태소 단위로 분리하고 품사 태그 부여 실습
- jamo를 사용하여 자모 단위로 한글을 쪼개는 실험을 진행하고 다시 복원하는 함수 구성
- 'jamo 기반으로 토큰을 나누는 방식은 왜 필요한가?'라는 질문을 통해 문장 분리 이상의 구조 이해 강화
- 각 형태소 분석기의 출력 비교를 통해 어떤 분석기가 어떤 상황에서 적합한지 실험
- TwitterTokenizer의 명사 추출 기능을 이용한 키워드 기반 문서 요약 시도

### 회고
한글은 영어와는 전혀 다른 전처리 접근이 필요하다는 점을 체감했고, 형태소 단위에서부터 의미를 어떻게 분리하고 추출할지를 고민하는 계기가 되었다.

## D0409

### 학습내용
NSMC(네이버 영화 리뷰) 데이터를 활용한 감성 분류 실습. 전처리, 토큰화, 단어 사전 구축부터 단순 RNN 모델 구현까지 전체 흐름을 실습함.

### 개인 복습/실습
- KoNLPy로 텍스트를 토큰화하고 pad_sequence로 길이를 통일하는 전처리 구성
- tokenizer를 통해 단어 사전을 구성하고 one-hot vector와 integer encoding을 비교 실습
- torchtext와 직접 구성한 데이터셋 class를 비교하며 NSMC를 학습셋/테스트셋으로 분할
- RNN 구조 내 임베딩 → 은닉 상태 → Linear → Sigmoid 흐름 구현
- 'RNN의 hidden state는 왜 반복적으로 입력되는가?', 'PackedSequence의 목적은?' 등 질문 기반 구조 학습

### 회고
감성 분류 모델을 처음으로 구성하면서, 전처리의 복잡함과 모델 구조가 맞물려 돌아가는 과정을 직접 눈으로 보고 설계 감각을 익힐 수 있었다.

## D0410

### 학습내용
EmbeddingBag을 활용한 텍스트 분류 실습을 진행하며, 다양한 전처리 방식과 임베딩의 효율성을 이해함. 불균형한 길이 문제와 텍스트 인코딩 기법의 차이도 비교함.

### 개인 복습/실습
- torch.nn.EmbeddingBag을 사용하여 문장 전체를 평균 임베딩하는 방식을 학습함
- sentence를 word index list로 변환하고 offset 설정을 통한 병렬 처리 구현
- 'EmbeddingBag과 Embedding의 차이는?', 'mean pooling의 효과는?'에 대한 질문을 코드로 실험함
- 한 문장이 갖는 정보의 분산 정도를 고려하여 embedding vector size를 다양하게 설정해 성능 변화 확인
- mini-batch 단위 분류기 구성 후 inference + 시각화 실험

### 회고
EmbeddingBag을 활용하면 속도는 빨라지지만 개별 단어 정보의 상호작용이 줄어든다는 점을 명확히 인지하게 되어, 단순 성능 외에 구조의 해석력도 중요하다는 생각이 들었다.

## D0411

### 학습내용
Seq2Seq 구조를 학습하며 Encoder-Decoder 모델을 설계함. Embedding → GRU → Decoder로 이어지는 구조와 hidden state 전달 흐름을 실습하고 직접 문장을 생성함.

### 개인 복습/실습
- input과 target을 각각 embedding + GRU로 구성한 Seq2Seq 모델 구현
- Teacher Forcing 기법을 적용하여 학습 안정성과 예측 품질 차이를 확인
- 'Decoder는 왜 input + hidden을 함께 받아야 하는가?', 'GRU vs LSTM 성능 차이는?' 등 질문 기반 분석
- torchtext 없이 데이터셋 전처리부터 토큰화, padding, index mapping 수작업 구현
- 학습된 모델로 직접 문장을 생성하고, 반복 구조 속 생성 문장 품질 차이를 시각화함

### 회고
Sequence 모델이 단어 단위 문맥을 어떻게 조정하는지를 처음으로 실험적으로 확인했고, 단순한 분류와 달리 구조적으로 생성이 가능하다는 점에서 큰 충격을 받았다.

## D0414

### 학습내용
IMDB 데이터셋을 활용하여 1D CNN 기반 텍스트 분류 모델을 구현함. 문장 길이를 고정하고 convolution + pooling 구조를 텍스트에 적용하는 새로운 시도 진행.

### 개인 복습/실습
- torchtext.datasets.IMDB에서 데이터를 불러와 전처리 후 vocab 구축 및 label 이진화
- 문장을 정해진 길이로 pad하고 conv1d + relu + maxpool + linear 구조 구현
- 'Conv1d에서 kernel size는 왜 문맥 창 역할을 할까?', 'CNN이 RNN보다 나은 이유는?' 등 구조 기반 질문 반복 학습
- 다양한 필터 수, kernel size를 바꿔 성능 실험하고 dropout 위치에 따른 일반화 성능 비교
- 테스트셋 정확도 시각화 및 misclassified case 출력까지 확장 실험 진행

### 회고
텍스트 분류에서 CNN을 적용한다는 것이 처음에는 어색했지만, 결국 단어 패턴을 감지하는 필터의 역할을 이해하며 단어 순서와 의미 구조를 동시에 반영할 수 있다는 점을 체감했다.
