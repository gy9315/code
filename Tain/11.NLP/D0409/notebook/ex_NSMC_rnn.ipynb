{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [ 한글 데이터셋  RNN ] <hr>\n",
    "- 데이터셋 : Kopora의 NAVER Sentiment Movie Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1] 데이터 준비 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ===> 모듈 로딩\n",
    "#%pip install Korpora\n",
    "\n",
    "from Korpora import Korpora \n",
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\gy931\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\gy931\\Korpora\\nsmc\\ratings_test.txt\n",
      "NSMCKorpus\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "NSMC.train: size=150000\n",
      "  - NSMC.train.texts : list[str]\n",
      "  - NSMC.train.labels : list[int]\n",
      "NSMC.test: size=50000\n",
      "  - NSMC.test.texts : list[str]\n",
      "  - NSMC.test.labels : list[int]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### ===> 데이터 로딩\n",
    "corpous = Korpora.load('nsmc')\n",
    "print(corpous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    50000 non-null  object\n",
      " 1   label   50000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "nsmcDF = pd.DataFrame(corpous.test)\n",
    "nsmcDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas(Index=0, text='굳 ㅋ', label=1) 굳 ㅋ 1\n"
     ]
    }
   ],
   "source": [
    "nsmIter=nsmcDF.itertuples()\n",
    "for item in nsmIter:\n",
    "    print(item, item.text, item.label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [2] 데이터셋 준비 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, corpus):\n",
    "        nsmcDF = pd.DataFrame(corpus).fillna('')\n",
    "\n",
    "        x_data = nsmcDF['text'].values\n",
    "        self.x_data = x_data\n",
    "        self.y_data = nsmcDF['label'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x_data[idx]\n",
    "        y = self.y_data[idx]\n",
    "        return y, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, texts, labels, vocab, tokenizer):\n",
    "#         super().__init__()\n",
    "#         self.texts = texts\n",
    "#         self.labels = labels\n",
    "#         self.vocab = vocab\n",
    "#         self.tokenizer = tokenizer\n",
    "        \n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "        \n",
    "#     def __getitem__(self, idx):\n",
    "#         text = self.texts.iloc[idx]\n",
    "#         label = self.labels.iloc[idx]\n",
    "#         return self.vocab(self.tokenizer(text)), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDS = CustomDataset(corpous.train)\n",
    "testDS = CustomDataset(corpous.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainDS =>  150000개   testDS =>  50000개\n"
     ]
    }
   ],
   "source": [
    "print(f'trainDS =>  {len(trainDS)}개   testDS =>  {len(testDS)}개')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 아 더빙.. 진짜 짜증나네요 목소리\n",
      "1 굳 ㅋ\n"
     ]
    }
   ],
   "source": [
    "for label, text in trainDS:\n",
    "    print(label, text)\n",
    "    break\n",
    "\n",
    "for label, text in testDS:\n",
    "    print(label, text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [2] 단어 사전 생성 <hr>\n",
    "    * 토큰화 진행 ==> 형태소 분석기 선택 \n",
    "    * 단어 사전 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [2-1] 토큰화 관련 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 모듈 로딩\n",
    "from konlpy.tag import Okt\n",
    "import torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "torchtext.disable_torchtext_deprecation_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 토큰관련 특별 문자\n",
    "UNK = '<UNK>'\n",
    "PAD = '<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 토큰화 인스턴스 생성\n",
    "tokenizer = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 토큰 제너레이터 함수 : 데이터 추출하여 토큰화 \n",
    "def yield_tokens(data_iter):\n",
    "    for label, text in data_iter:\n",
    "        # 라벨, 텍스트 --> 텍스트 토큰화\n",
    "        yield tokenizer.morphs(text, stem=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [2-2] 토큰화 ===> 단어/어휘 사전 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 토큰화 및 단어/어휘 사전 생성\n",
    "VOCAB = build_vocab_from_iterator(\n",
    "    yield_tokens(trainDS),\n",
    "    min_freq=2,\n",
    "    specials= [PAD, UNK],\n",
    "    special_first=True\n",
    ")\n",
    "\n",
    "### <UNK> 인덱스 설정\n",
    "VOCAB.set_default_index(VOCAB[UNK])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<PAD>', '<UNK>', '.', '이', '영화', '보다', '하다', '의', '..'], 4, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### VOCAB 메서드 \n",
    "VOCAB.get_itos()[:9], VOCAB.get_stoi()['영화'], VOCAB.get_stoi()[UNK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 텍스트 >>>> 정수 인코딩\n",
    "text_pipeline = lambda x: VOCAB(tokenizer.morphs(text, stem=True))\n",
    "\n",
    "### ===> 레이틀 >>> 정수 인코딩 (0~3)\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [2-3] 인코딩 & 디코딩 인덱싱 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 인코딩 : 문자 >>>> 숫자로 변환\n",
    "token_to_id ={ label : id  for label, id in VOCAB.get_stoi().items()}\n",
    "\n",
    "### 디코딩 : 숫자 >>>> 문자로 변환\n",
    "id_to_token ={ id : label  for label, id in VOCAB.get_stoi().items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [3] 데이터 로더 생성 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 모듈로딩\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 실행 디바이스 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch,vocab):\n",
    "    label_list, target_list= [], []\n",
    "    for label,target in batch:\n",
    "         label_list.append(vocab[label])\n",
    "         processed_text = torch.tensor(target, dtype=torch.int64)\n",
    "         target_list.append(processed_text)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.float32)\n",
    "    target_list = pad_sequence(target_list, batch_first=True, padding_value=0)\n",
    "    return label_list.to(DEVICE), target_list.to(DEVICE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " num_train :142500\n",
      " len(split_trainDS) :142500\n",
      " len(split_validDS) :7500\n"
     ]
    }
   ],
   "source": [
    "### ===> 학습용, 검증용, 테스트용 DataSet 준비 \n",
    "BATCH_SIZE = 64\n",
    "\n",
    "### 학습용, 검증용, 테스트용 Dataset, DataLoader 준비\n",
    "num_train = int(len(trainDS) * 0.95)\n",
    "print(f' num_train :{num_train}')\n",
    "\n",
    "split_trainDS, split_validDS= random_split( trainDS, [num_train, len(trainDS) - num_train])\n",
    "print(f' len(split_trainDS) :{len(split_trainDS)}')\n",
    "print(f' len(split_validDS) :{len(split_validDS)}')\n",
    "\n",
    "trainDL = DataLoader( split_trainDS, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch ,generator=)\n",
    "validDL = DataLoader( split_validDS, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch )\n",
    "testDL  = DataLoader( testDS,        batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " len(trainDL) :142528\n",
      " len(validDL) :7552\n",
      " len(testDL) :50048\n"
     ]
    }
   ],
   "source": [
    "print(f' len(trainDL) :{len(trainDL)*BATCH_SIZE}')\n",
    "print(f' len(validDL) :{len(validDL)*BATCH_SIZE}')\n",
    "print(f' len(testDL) :{len(testDL)* BATCH_SIZE}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
